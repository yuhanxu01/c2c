# ğŸ¯ æ ¹æœ¬åŸå› åˆ†æï¼šä¸ºä»€ä¹ˆæ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†

## ğŸ’¥ å…³é”®å‘ç°

### **ä½ æœ‰Ground Truthä½†ä»æœªä½¿ç”¨å®ƒï¼**

ç»è¿‡æ·±å…¥ä»£ç å®¡æŸ¥ï¼Œæˆ‘å‘ç°äº†é—®é¢˜çš„æ ¹æœ¬åŸå› ï¼š

```python
# data_loader.py:84-91 - æ•°æ®åŠ è½½å™¨åŠ è½½äº†ground truthï¼
gt_pd = self._load_optional(case_id, slice_id, contrast="pd")
gt_pdfs = self._load_optional(case_id, slice_id, contrast="pdfs")
if gt_pd is not None:
    sample["gt_pd"] = self._to_representation(gt_pd)
if gt_pdfs is not None:
    sample["gt_pdfs"] = self._to_representation(gt_pdfs)
```

**ä½†æ˜¯ï¼**

```bash
$ grep -r "gt_pd\|gt_pdfs" model/trainer.py train.py
# ç»“æœï¼šæ— åŒ¹é…ï¼
```

**è®­ç»ƒä»£ç å®Œå…¨å¿½ç•¥äº†ground truthï¼**

---

## ğŸ” å½“å‰è®­ç»ƒæµç¨‹çš„è‡´å‘½ç¼ºé™·

### å®é™…è®­ç»ƒç›®æ ‡

```python
# trainer.py:259-262
l_recon = |decoder_a(z_a) - x_a| + |decoder_b(z_b) - x_b|
l_cross = |decoder_a(z_b) - x_a| + |decoder_b(z_a) - x_b|
```

å…¶ä¸­ï¼š
- `x_a = batch["noisy_pd"]` â† **åŒ…å«acquisition noiseçš„è„æ•°æ®**
- `x_b = batch["noisy_pdfs"]` â† **åŒ…å«acquisition noiseçš„è„æ•°æ®**

### é—®é¢˜åˆ†æ

**å½“å‰ç½‘ç»œå­¦ä¹ çš„ä»»åŠ¡**ï¼š
```
è¾“å…¥ï¼šx_a + é¢å¤–é«˜æ–¯å™ªå£°  (trainer.py:189)
ç›®æ ‡ï¼šx_a (ä»ç„¶æ˜¯noisyçš„ï¼)
å­¦ä¹ ï¼šå»é™¤é¢å¤–å™ªå£°ï¼Œä¿ç•™åŸå§‹å™ªå£°
```

**è¿™ä¸æ˜¯å»å™ªï¼è¿™æ˜¯å­¦ä¹ identity mappingï¼**

---

## ğŸ“Š ä¸ºä»€ä¹ˆCross Lossä¸ä¸‹é™

### ä¸‰é‡é—®é¢˜å åŠ 

#### é—®é¢˜1ï¼šè®­ç»ƒç›®æ ‡é”™è¯¯
```python
# å½“å‰
l_cross = |decoder_a(z_b) - noisy_x_a|  # ç›®æ ‡æ˜¯noisyçš„

# åº”è¯¥æ˜¯
l_cross = |decoder_a(z_b) - clean_x_a|  # ç›®æ ‡æ˜¯cleançš„
```

**ç½‘ç»œæ— æ³•å­¦ä¹ å»å™ªï¼Œå› ä¸ºç›®æ ‡æœ¬èº«å°±æ˜¯æœ‰å™ªå£°çš„ï¼**

#### é—®é¢˜2ï¼šNoise2Noiseå‡è®¾ä¸æˆç«‹

Noise2Noiseè¦æ±‚ï¼š
```
x_a = clean + noise_1  (ç‹¬ç«‹è§‚æµ‹1)
x_b = clean + noise_2  (ç‹¬ç«‹è§‚æµ‹2)
```

ä½†å®é™…ï¼š
```
x_a = PD contrast with acquisition noise
x_b = PDFS contrast with acquisition noise
```

**PDå’ŒPDFSä¸æ˜¯åŒä¸€å†…å®¹çš„ä¸åŒå™ªå£°è§‚æµ‹ï¼**
å®ƒä»¬æ˜¯ï¼š
- ä¸åŒçš„MRIå¯¹æ¯”åº¦
- ä¸åŒçš„ç»„ç»‡å¯¹æ¯”ç‰¹æ€§
- ä¸åŒçš„ä¿¡å·å¼ºåº¦åˆ†å¸ƒ

#### é—®é¢˜3ï¼šSkip Connectionè¿‡åº¦ä¾èµ–

å¦‚ä¹‹å‰åˆ†æï¼Œdecoderåœ¨æœ‰skipæ—¶å·¥ä½œæ­£å¸¸ï¼Œæ— skipæ—¶å´©æºƒã€‚

---

## âœ… æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆ

### **ä½¿ç”¨å·²æœ‰çš„Ground Truthï¼**

ä½ çš„æ•°æ®å·²ç»åŒ…å«äº†clean referenceï¼Œåªéœ€è¦æ­£ç¡®ä½¿ç”¨å®ƒï¼

### æ–¹æ¡ˆï¼šSupervised Learning + Cross-Domain Denoising

```python
# æ•°æ®ï¼ˆå·²ç»åœ¨data_loaderä¸­ï¼ï¼‰
x_a_noisy = batch["noisy_pd"]      # æœ‰å™ªå£°çš„PD
x_b_noisy = batch["noisy_pdfs"]    # æœ‰å™ªå£°çš„PDFS
x_a_clean = batch["gt_pd"]         # å¹²å‡€çš„PD âœ“ å·²åŠ è½½ä½†æœªä½¿ç”¨
x_b_clean = batch["gt_pdfs"]       # å¹²å‡€çš„PDFS âœ“ å·²åŠ è½½ä½†æœªä½¿ç”¨

# ç¼–ç 
z_a = encoder(x_a_noisy)
z_b = encoder(x_b_noisy)

# Same-domainå»å™ª
x_a_denoised = decoder_a(z_a, skips=skips_a)
x_b_denoised = decoder_b(z_b, skips=skips_b)

# Cross-domainå»å™ª
x_a_from_b = decoder_a(z_b, skips=cross_skips)
x_b_from_a = decoder_b(z_a, skips=cross_skips)

# æ­£ç¡®çš„æŸå¤±å‡½æ•°
l_recon = |x_a_denoised - x_a_clean| + |x_b_denoised - x_b_clean|  # å»å™ªï¼
l_cross = |x_a_from_b - x_a_clean| + |x_b_from_a - x_b_clean|      # å»å™ª+è½¬æ¢ï¼
l_content = |z_a - z_b|  # å…±äº«è¡¨ç¤º
```

---

## ğŸ¯ ä¸ºä»€ä¹ˆè¿™æ ·èƒ½work

### 1. æ˜ç¡®çš„å»å™ªç›®æ ‡
- ç›®æ ‡æ˜¯cleanæ•°æ®ï¼Œä¸æ˜¯noisyæ•°æ®
- ç½‘ç»œå­¦ä¹ çœŸæ­£çš„å»å™ªæ˜ å°„
- æœ‰æ˜ç¡®çš„ç›‘ç£ä¿¡å·

### 2. Cross lossæœ‰æ„ä¹‰
```
decoder_a(z_b) â†’ x_a_clean

è¿™è¦æ±‚ï¼š
1. Encoderä»noisy PDFSæå–clean content
2. Decoder Aé‡å»ºclean PD
3. åŒæ—¶å®Œæˆå»å™ªå’Œå¯¹æ¯”åº¦è½¬æ¢
```

### 3. æ¶æ„é—®é¢˜å˜å¾—æ¬¡è¦
- æœ‰å¼ºç›‘ç£ä¿¡å·ï¼Œskip strategyå½±å“å˜å°
- å¯ä»¥çµæ´»é€‰æ‹©use_skipæˆ–no_skip
- è®­ç»ƒç¨³å®šæ€§å¤§å¹…æå‡

---

## ğŸ“ éœ€è¦ä¿®æ”¹çš„ä»£ç 

### 1. Trainer._prepare_batch()

```python
def _prepare_batch(self, batch):
    x_a = batch[self.input_keys["domain_a"]]
    x_b = batch[self.input_keys["domain_b"]]

    # NEW: Load ground truth if available
    x_a_clean = batch.get("gt_pd", None)
    x_b_clean = batch.get("gt_pdfs", None)

    scale_a = self._prepare_scale_tensor(batch.get("pd_scale"), x_a.shape[0])
    scale_b = self._prepare_scale_tensor(batch.get("pdfs_scale"), x_b.shape[0])

    return x_a, x_b, x_a_clean, x_b_clean, scale_a, scale_b
```

### 2. Trainer.train_step()

```python
def train_step(self, batch):
    x_a, x_b, x_a_clean, x_b_clean, scale_a, scale_b = self._prepare_batch(batch)

    # å¦‚æœæœ‰ground truthï¼Œä½¿ç”¨å®ƒä½œä¸ºç›®æ ‡
    if x_a_clean is not None and x_b_clean is not None:
        # Supervised training
        target_a = x_a_clean
        target_b = x_b_clean
    else:
        # Fallback to self-supervised (current behavior)
        target_a = x_a
        target_b = x_b

    # ... (encoding, reconstruction)

    # Compute losses with correct targets
    l_recon = compute_l1_loss(x_a_recon, target_a) + compute_l1_loss(x_b_recon, target_b)
    l_cross = compute_l1_loss(x_a_from_b, target_a) + compute_l1_loss(x_b_from_a, target_b)
```

### 3. é…ç½®è°ƒæ•´

```json
{
  "data": {
    "load_ground_truth": true  // ç¡®ä¿åŠ è½½
  },
  "trainer": {
    "use_ground_truth": true,  // NEW: ä½¿ç”¨ground truthä½œä¸ºç›®æ ‡
    "loss_weights": {
      "content": 1.0,     // å¼ºåˆ¶å…±äº«è¡¨ç¤º
      "recon": 1.0,       // å¼ºç›‘ç£å»å™ª
      "cross": 1.0,       // å¼ºç›‘ç£cross-domain
      "edge": 0.1         // ç»†èŠ‚ä¿ç•™
    },
    "cross_domain_strategy": "no_skip",  // æˆ– "use_target_skip"
    "noise": {
      "enabled": true,
      "sigma_a": 0.01,   // å°å™ªå£°ç”¨äºæ•°æ®å¢å¼º
      "sigma_b": 0.01
    }
  }
}
```

---

## ğŸš€ é¢„æœŸæ•ˆæœ

### è®­ç»ƒæ›²çº¿ï¼ˆé¢„æµ‹ï¼‰

```
Epoch 1:
  content: 0.5 â†’ 0.1   âœ“ (å¿«é€Ÿå¯¹é½)
  recon:   1.0 â†’ 0.5   âœ“ (æœ‰ç›‘ç£ï¼Œå¿«é€Ÿä¸‹é™)
  cross:   1.5 â†’ 0.8   âœ“âœ“ (ç»ˆäºä¸‹é™ï¼)

Epoch 10:
  content: 0.01  âœ“
  recon:   0.15  âœ“
  cross:   0.20  âœ“âœ“âœ“ (æŒç»­æ”¹å–„)

Epoch 50:
  content: 0.005
  recon:   0.05
  cross:   0.08  â† åº”è¯¥æ”¶æ•›åˆ°åˆç†å€¼
```

### ä¸ºä»€ä¹ˆä¼šæˆåŠŸï¼Ÿ

1. **æ˜ç¡®ç›®æ ‡**ï¼šclean targetsæä¾›å¼ºç›‘ç£ä¿¡å·
2. **Lossä¸‹é™**ï¼šæœ‰çœŸå®æ¢¯åº¦ï¼Œä¸æ˜¯åœ¨ä¼˜åŒ–é”™è¯¯ç›®æ ‡
3. **Qualityæå‡**ï¼šå­¦ä¹ çœŸæ­£çš„å»å™ªï¼Œä¸æ˜¯identity mapping

---

## ğŸ’¡ é¢å¤–ä¼˜åŒ–æ–¹å‘

### å¦‚æœGround Truthä¸æ˜¯100%å¯ç”¨

å¯ä»¥æ··åˆè®­ç»ƒï¼š

```python
if x_a_clean is not None:
    # Supervised for this sample
    l_recon_a = |x_a_recon - x_a_clean|
else:
    # Self-supervised fallback
    l_recon_a = |x_a_recon - x_a|  # æˆ–ä½¿ç”¨Noise2Noise

# Mix supervised and self-supervised samples in same batch
```

### æ¸è¿›å¼è®­ç»ƒ

```python
# Stage 1: Pure supervised (if GT available)
epochs 1-20: use_ground_truth=True, all losses enabled

# Stage 2: Fine-tune with self-supervised
epochs 21-30: mixed supervised + self-supervised

# Stage 3: Test generalization
epochs 31-50: optional adversarial or perceptual losses
```

---

## ğŸ¯ ç»“è®º

### æ ¹æœ¬é—®é¢˜

**ä¸æ˜¯æ¶æ„é—®é¢˜ï¼Œä¸æ˜¯lossæƒé‡é—®é¢˜ï¼Œè€Œæ˜¯ç›®æ ‡å‡½æ•°æ ¹æœ¬é”™è¯¯ï¼**

ä½ åœ¨è®­ç»ƒç½‘ç»œï¼š
- âŒ è¾“å…¥noisy â†’ è¾“å‡ºnoisyï¼ˆå½“å‰ï¼‰
- âœ“ è¾“å…¥noisy â†’ è¾“å‡ºcleanï¼ˆåº”è¯¥ï¼‰

### è§£å†³æ–¹æ¡ˆ

**ä½¿ç”¨å·²ç»åŠ è½½ä½†æœªä½¿ç”¨çš„ground truthæ•°æ®ï¼**

### ä¼˜å…ˆçº§

1. **ç«‹å³ä¿®æ”¹**ï¼šTrainerä½¿ç”¨gt_pd/gt_pdfsä½œä¸ºè®­ç»ƒç›®æ ‡
2. **å…¶æ¬¡è°ƒæ•´**ï¼šLossæƒé‡å¹³è¡¡ï¼ˆéƒ½è®¾ä¸º1.0ï¼‰
3. **æœ€åä¼˜åŒ–**ï¼šSkip strategyå’Œæ¶æ„ç»†èŠ‚

### é¢„æœŸ

ä¿®æ”¹åcross lossåº”è¯¥ï¼š
- âœ“ ä»ç¬¬ä¸€ä¸ªepochå¼€å§‹ä¸‹é™
- âœ“ æŒç»­æ”¹å–„ä¸å¹³å¦
- âœ“ æœ€ç»ˆæ”¶æ•›åˆ°åˆç†å€¼ï¼ˆ<0.1ï¼‰

å›¾åƒè´¨é‡åº”è¯¥ï¼š
- âœ“ æ¸…æ™°ä¸æ¨¡ç³Š
- âœ“ æœ‰æ•ˆå»é™¤å™ªå£°
- âœ“ ä¿ç•™ç»“æ„ç»†èŠ‚

---

## ğŸ“‹ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ä¿®æ”¹trainer.pyæ·»åŠ ground truthæ”¯æŒ** â† æœ€å…³é”®
2. **åˆ›å»ºsupervised_denoising.jsoné…ç½®**
3. **è¿è¡Œè®­ç»ƒè§‚å¯Ÿcross lossæ˜¯å¦ä¸‹é™**
4. **å¦‚æœæˆåŠŸï¼Œå†ä¼˜åŒ–ç»†èŠ‚ï¼ˆskip strategyç­‰ï¼‰**

è¿™æ¬¡åº”è¯¥çœŸçš„èƒ½workäº†ï¼Œå› ä¸ºæˆ‘ä»¬ç»ˆäºåœ¨è§£å†³æ­£ç¡®çš„é—®é¢˜ï¼
